# agent_nodes.py

import os
import json
import random
from typing import Annotated, Literal, Sequence, TypedDict, List, Dict, Any
from langchain_core.messages import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser, JsonOutputParser
from langchain_openai import ChatOpenAI

# utils.pyì—ì„œ ì¶”ì¶œ í•¨ìˆ˜ ì„í¬íŠ¸
from utils import extract_text_from_file

# --- í™˜ê²½ ì„¤ì • ---
LLM_MODEL = "gpt-4o-mini"
# OPENAI_API_KEYëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œëœë‹¤ê³  ê°€ì •
llm = ChatOpenAI(model=LLM_MODEL, temperature=0)

# --- State ì •ì˜ (ëª¨ë“  ì •ë³´ í¬í•¨) ---
class InterviewState(TypedDict):
    # ê³ ì • ì •ë³´
    resume_text: str
    resume_summary: str
    resume_keywords: List[str]
    question_strategy: Dict[str, Dict]
    
    # ë©´ì ‘ ì§„í–‰ ìƒíƒœ
    current_question: str
    current_answer: str
    current_strategy: str # í˜„ì¬ ì§ˆë¬¸ì˜ ì „ëµ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: 'ê²½ë ¥ ë° ê²½í—˜')
    
    # --- ê³ ë„í™” í•­ëª© ---
    question_queue: List[str] # í˜„ì¬ ì£¼ì œì˜ ë‚¨ì€ ì˜ˆì‹œ ì§ˆë¬¸ (Popìœ¼ë¡œ ì†Œì§„)
    remaining_topics: List[str] # ë‚¨ì•„ìˆëŠ” ì£¼ì œ ì¹´í…Œê³ ë¦¬ (Popìœ¼ë¡œ ì†Œì§„)
    generate_count: int # ì‹¬í™” ì§ˆë¬¸ ìƒì„± íšŸìˆ˜ ì¹´ìš´í„°

    # ì¸í„°ë·° ë¡œê·¸
    conversation: List[Dict[str, str]]
    evaluation : List[Dict[str, str]]
    next_step : str # "generate", "summarize", "next_topic", "end" ì¤‘ í•˜ë‚˜

# ===============================
# ğŸ”¹ Node Functions
# ===============================

def analyze_resume(state: InterviewState) -> InterviewState:
    """ì´ë ¥ì„œ ë¶„ì„: ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ"""
    resume_text = state.get('resume_text')
    if not resume_text:
      return {**state, "resume_summary": "ì´ë ¥ì„œ í…ìŠ¤íŠ¸ ì—†ìŒ", "resume_keywords": []}

    # 1. ìš”ì•½ ì¶”ì¶œ ì²´ì¸
    summary_prompt = ChatPromptTemplate.from_template(
        "\"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ ì±„ìš© ë‹´ë‹¹ìì…ë‹ˆë‹¤. ë‹¤ìŒ ì´ë ¥ì„œ í…ìŠ¤íŠ¸ë¥¼ 3-4ì¤„ì˜ í•µì‹¬ ë‚´ìš©ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n        ---\n        {resume}\n        \"\"\""
    )
    summary_chain = summary_prompt | llm | StrOutputParser()
    resume_summary = summary_chain.invoke({"resume": resume_text})

    # 2. í‚¤ì›Œë“œ ì¶”ì¶œ ì²´ì¸
    keywords_prompt = ChatPromptTemplate.from_template(
        "\"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ IT í—¤ë“œí—Œí„°ì…ë‹ˆë‹¤. ë‹¤ìŒ ì´ë ¥ì„œ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œ 10ê°œë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.\n        ì˜ˆ: Python, ë°ì´í„° ë¶„ì„, NLP, í”„ë¡œì íŠ¸ ê´€ë¦¬, ë¦¬ë”ì‹­\n        ---\n        {resume}\n        \"\"\""
    )
    keywords_chain = keywords_prompt | llm | CommaSeparatedListOutputParser()
    resume_keywords = keywords_chain.invoke({"resume": resume_text})

    return {
        **state,
        "resume_summary": resume_summary,
        "resume_keywords": resume_keywords,
    }

def generate_question_strategy(state: InterviewState) -> InterviewState:
    """ì§ˆë¬¸ ì „ëµ ìˆ˜ë¦½: 3ê°€ì§€ ë¶„ì•¼ì˜ ì§ˆë¬¸ ë°©í–¥ê³¼ ì˜ˆì‹œ ì§ˆë¬¸ì„ JSONìœ¼ë¡œ ìƒì„±"""
    resume_summary = state.get('resume_summary')
    resume_keywords = state.get('resume_keywords')
    
    parser = JsonOutputParser()

    prompt = ChatPromptTemplate.from_template(
        "\"\"\"ë‹¹ì‹ ì€ AI ë©´ì ‘ê´€ì˜ ì§ˆë¬¸ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ ì´ë ¥ì„œ ìš”ì•½ê³¼ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ 3ê°€ì§€ ì£¼ìš” ì¹´í…Œê³ ë¦¬(ê²½ë ¥ ë° ê²½í—˜, ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëŠ¥ë ¥, ë…¼ë¦¬ì  ì‚¬ê³ )ì— ëŒ€í•œ ë©´ì ‘ ì§ˆë¬¸ ì „ëµì„ ìˆ˜ë¦½í•´ ì£¼ì„¸ìš”.\n"
        "\n"
        "ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ \"ì§ˆë¬¸ ë°©í–¥\"ê³¼ 2ê°œì˜ \"ì˜ˆì‹œ ì§ˆë¬¸\" ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "{format_instructions}\n"
        "\n"
        "--- ì´ë ¥ì„œ ìš”ì•½ ---\n"
        "{summary}\n"
        "\n"
        "--- í•µì‹¬ í‚¤ì›Œë“œ ---\n"
        "{keywords}\n"
        "\"\"\"",
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )

    chain = prompt | llm | parser
    
    strategy_dict = chain.invoke({'summary': resume_summary, 'keywords': resume_keywords})
    
    # ë‚¨ì•„ìˆëŠ” ì£¼ì œ ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™”
    remaining_topics = list(strategy_dict.get("ë©´ì ‘ ì§ˆë¬¸ ì „ëµ", {}).keys())
    
    # ì²« ì§ˆë¬¸ ë° í ì´ˆê¸°í™” (ê²½ë ¥ ë° ê²½í—˜ ì¹´í…Œê³ ë¦¬ë¡œ ì‹œì‘)
    first_topic = "ê²½ë ¥ ë° ê²½í—˜"
    first_question = ""
    question_queue = []
    current_strategy = ""
    
    if first_topic in strategy_dict.get("ë©´ì ‘ ì§ˆë¬¸ ì „ëµ", {}):
        questions = strategy_dict["ë©´ì ‘ ì§ˆë¬¸ ì „ëµ"][first_topic].get("ì˜ˆì‹œ ì§ˆë¬¸", [])
        if questions:
            first_question = questions[0]
            question_queue = questions[1:] # ì²« ì§ˆë¬¸ ì œì™¸í•œ ë‚˜ë¨¸ì§€ëŠ” íì—
            current_strategy = first_topic
            # remaining_topicsì—ì„œ ì²« ì£¼ì œ ì œê±°
            if first_topic in remaining_topics:
                remaining_topics.remove(first_topic)


    return {
        **state,
        "question_strategy": strategy_dict,
        "current_question": first_question if first_question else "ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
        "current_strategy": current_strategy if current_strategy else "ììœ  ì£¼ì œ",
        "question_queue": question_queue,
        "remaining_topics": remaining_topics,
        "generate_count": 0,
    }


def evaluate_answer(state: InterviewState) -> InterviewState:
    """ë‹µë³€ í‰ê°€: ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±, êµ¬ì²´ì„± ë“± 2ê°œ í•­ëª©ìœ¼ë¡œ LLM í‰ê°€ ìˆ˜í–‰ (ì ìˆ˜ ë° ì˜ê²¬ í¬í•¨)"""
    current_question = state.get("current_question")
    current_answer = state.get("current_answer")
    
    # conversation ì—…ë°ì´íŠ¸ëŠ” ì—¬ê¸°ì„œ ìˆ˜í–‰ (ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ì§ì§€ì–´ì§ˆ ë•Œ)
    conversation = state.get("conversation", [])
    conversation.append({"question": current_question, "answer": current_answer})
    
    evaluation = state.get("evaluation", [])
    
    # NOTE: ë¯¸ì…˜2ì—ì„œ í‰ê°€ í•­ëª©ì„ 5ê°œë¡œ ê³ ë„í™”í•  ë•Œ ì´ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • í•„ìš”
    prompt = ChatPromptTemplate.from_template(
        "\"\"\"ë‹¹ì‹ ì€ AI ë©´ì ‘ê´€ì˜ ë‹µë³€ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
        "ì£¼ì–´ì§„ ë©´ì ‘ ì§ˆë¬¸ê³¼ ì§€ì›ìì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‘ ê°€ì§€ í•­ëª©ì— ëŒ€í•´ 'ìƒ', 'ì¤‘', 'í•˜' ì¤‘ í•˜ë‚˜ë¡œ í‰ê°€í•˜ê³  ê°„ë‹¨í•œ í‰ê°€ ì˜ê²¬ì„ ì‘ì„±í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.\n"
        "\n"
        "JSON í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤:\n"
        "{{\n"
        "    \"ì—°ê´€ì„±\": \"'ìƒ', 'ì¤‘', 'í•˜' ì¤‘ í•˜ë‚˜\",\n"
        "    \"êµ¬ì²´ì„±\": \"'ìƒ', 'ì¤‘', 'í•˜' ì¤‘ í•˜ë‚˜\",\n"
        "    \"í‰ê°€_ì˜ê²¬\": \"ê°„ë‹¨í•œ í‰ê°€ ì˜ê²¬ (ë¬¸ìì—´)\"\n"
        "}}\n"
        "\n"
        "í‰ê°€ ê¸°ì¤€:\n"
        "1. ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±: ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³  ê´€ë ¨ ë‚´ìš©ì„ ë‹µë³€í–ˆëŠ”ê°€?\n"
        "2. ë‹µë³€ì˜ êµ¬ì²´ì„±: ê²½í—˜ì´ë‚˜ ìƒê°ì„ êµ¬ì²´ì ì¸ ì‚¬ë¡€ë‚˜ ê·¼ê±°ë¥¼ ë“¤ì–´ ì„¤ëª…í–ˆëŠ”ê°€?\n"
        "\n"
        "--- ë©´ì ‘ ì§ˆë¬¸ ---\n"
        "{question}\n"
        "\n"
        "--- ì§€ì›ì ë‹µë³€ ---\n"
        "{answer}\n"
        "\"\"\""
    )
    
    chain = prompt | llm | JsonOutputParser()

    eval_result = chain.invoke({"question": current_question, "answer": current_answer})
    
    # í‰ê°€ ê¸°ë¡ ì—…ë°ì´íŠ¸
    evaluation.append({"question": current_question, "answer": current_answer, "evaluation": eval_result})
    
    return {
        **state,
        "conversation": conversation, # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        "evaluation": evaluation
    }


def decide_next_step(state: InterviewState) -> InterviewState:
    """ë‹¤ìŒ ë‹¨ê³„ ê²°ì • (ì¢…ë£Œ, ì£¼ì œ ì „í™˜, ë‹¤ìŒ ì§ˆë¬¸, ì‹¬í™” ì§ˆë¬¸)"""
    
    conversation_count = len(state.get("conversation", []))
    question_queue = state.get("question_queue", [])
    remaining_topics = state.get("remaining_topics", [])
    evaluation = state.get("evaluation", [])
    generate_count = state.get("generate_count", 0)

    # 1. LLM í˜¸ì¶œ ì¤€ë¹„: ìµœê·¼ í‰ê°€ ê²°ê³¼ì™€ ì§ˆë¬¸ ê¸°ë¡ ì •ë¦¬
    if not evaluation:
        return {**state, "next_step": "generate"} # í‰ê°€ê°€ ì—†ìœ¼ë©´ ì¼ë‹¨ ì§ˆë¬¸ ìƒì„±ìœ¼ë¡œ ë³´ëƒ„
        
    last_eval_item = evaluation[-1]
    last_evaluation_dict = last_eval_item.get("evaluation", {})
    
    # ì—°ê´€ì„±/êµ¬ì²´ì„± ë“±ê¸‰ì„ ë¬¸ìì—´ë¡œ ì¶”ì¶œ (LLMì—ê²Œ ì „ë‹¬í•˜ê¸° ìœ„í•¨)
    eval_summary = (
        f"ì—°ê´€ì„±: {last_evaluation_dict.get('ì—°ê´€ì„±', 'ì¤‘')}, "
        f"êµ¬ì²´ì„±: {last_evaluation_dict.get('êµ¬ì²´ì„±', 'ì¤‘')}"
    )

    # LLM í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_template(
        "\"\"\"ë‹¹ì‹ ì€ AI ë©´ì ‘ ì§„í–‰ ê´€ë¦¬ìì…ë‹ˆë‹¤.\n"
        "ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€ í‰ê°€, ë‚¨ì€ ì˜ˆì‹œ ì§ˆë¬¸ ê°œìˆ˜, ë‚¨ì€ ì£¼ì œ ëª©ë¡ì„ ë³´ê³  ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•´ ì£¼ì„¸ìš”.\n"
        "\n"
        "[ê²°ì • ê·œì¹™]\n"
        "1. (ìµœìš°ì„ ) í˜„ì¬ ëŒ€í™” íšŸìˆ˜ê°€ {count}ì¸ë° ìµœëŒ€ 5ë²ˆì„ ë„˜ìœ¼ë©´ ì•ˆë©ë‹ˆë‹¤. 5ë²ˆ ì´ìƒì´ë©´ 'summarize'ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.\n"
        "2. (2ìˆœìœ„: ì‹¬í™” ì§ˆë¬¸) **ìµœê·¼ ë‹µë³€ í‰ê°€ì˜ 'ì—°ê´€ì„±' ë˜ëŠ” 'êµ¬ì²´ì„±'ì´ 'í•˜' ì´ë©´:** ì´ ì£¼ì œë¥¼ ì‹¬í™”í•˜ê¸° ìœ„í•´ 'generate'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (ì‹¬í™” ì§ˆë¬¸ ìƒì„± ë…¸ë“œ)\n"
        "3. (3ìˆœìœ„: ë‹¤ìŒ ì˜ˆì‹œ ì§ˆë¬¸) **í˜„ì¬ ì£¼ì œì˜ 'ë‚¨ì€ ì˜ˆì‹œ ì§ˆë¬¸'ì´ [ìˆìŒ]** ì´ë©´: 'next_question'ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n"
        "4. (4ìˆœìœ„: ì£¼ì œ ì „í™˜) **2, 3ë²ˆ ê·œì¹™ì— í•´ë‹¹í•˜ì§€ ì•Šê³ , ë‚¨ì€ ì£¼ì œ ëª©ë¡ì´ [ìˆìŒ]** ì´ë¼ë©´: ë‹¤ìŒ ì£¼ì œë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•´ 'next_topic'ì„ ë°˜í™˜í•©ë‹ˆë‹¤. (ì£¼ì œ ì „í™˜ ë…¸ë“œ)\n"
        "5. (5ìˆœìœ„: ì¢…ë£Œ) **ë‚¨ì€ ì£¼ì œ ëª©ë¡ì´ [ì—†ìŒ]** ì´ë¼ë©´: 'summarize'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n"
        "\n"
        "ë‹¹ì‹ ì˜ ê²°ì •ì€ ì˜¤ì§ 'generate', 'next_question', 'next_topic', 'summarize' ë„¤ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n"
        "\n"
        "--- í˜„ì¬ ìƒíƒœ ---\n"
        "1. í˜„ì¬ ëŒ€í™” íšŸìˆ˜: {count}\n"
        "2. ìµœê·¼ ë‹µë³€ í‰ê°€: {eval_summary}\n"
        "3. í˜„ì¬ ì£¼ì œì˜ ë‚¨ì€ ì˜ˆì‹œ ì§ˆë¬¸ ê°œìˆ˜: {queue_count}\n"
        "4. ë‚¨ì€ ì£¼ì œ ëª©ë¡: {remaining_topics}\n"
        "\n"
        "--- ë‹¤ìŒ í–‰ë™ (ì˜¤ì§ ë‹¨ì–´ í•˜ë‚˜ë¡œ ì‘ë‹µ) ---"
        "\"\"\""
    )
    
    chain = prompt | llm | StrOutputParser()
    
    # LLM í˜¸ì¶œ
    next_action = chain.invoke({
        "count": conversation_count,
        "eval_summary": eval_summary,
        "queue_count": len(question_queue),
        "remaining_topics": remaining_topics
    }).strip().lower()

    # 2. LLM ì‘ë‹µ ê¸°ë°˜ ë¶„ê¸° ì²˜ë¦¬
    if conversation_count >= 5: # ê·œì¹™ 1: 5íšŒ ì´ˆê³¼ ì‹œ ê°•ì œ ì¢…ë£Œ
        next_action = "summarize"
        
    elif next_action == "next_question": # ê·œì¹™ 3: ë‹¤ìŒ ì˜ˆì‹œ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™
        # next_question ë…¸ë“œë¡œ ë¶„ê¸°í•˜ê¸° ìœ„í•œ next_step ì„¤ì •.
        next_action = "next_question" 
        
    elif next_action == "next_topic": # ê·œì¹™ 4: ë‹¤ìŒ ì£¼ì œë¡œ ì „í™˜
        # next_topic_question ë…¸ë“œë¡œ ë¶„ê¸°í•˜ê¸° ìœ„í•œ next_step ì„¤ì •.
        next_action = "next_topic"
        
    elif next_action == "summarize": # ê·œì¹™ 5: ì¢…ë£Œ
        pass # summarizeë¡œ END

    # ê·œì¹™ 2: ì‹¬í™” ì§ˆë¬¸ (LLMì´ 'generate'ë¥¼ ë°˜í™˜í–ˆê±°ë‚˜, ì‹¬í™”ê°€ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ëœ ê²½ìš°)
    # NOTE: LLMì´ 'í•˜'ë¥¼ ë³´ê³  'generate'ë¥¼ ë°˜í™˜í–ˆì„ ë•Œ generate_countë¥¼ ì¦ê°€ì‹œí‚¤ë„ë¡ ë¡œì§ì„ ë¶„ë¦¬
    else: # next_action == "generate" (ì‹¬í™” ì§ˆë¬¸)
        state['generate_count'] = generate_count + 1
        # generate_question ë…¸ë“œë¡œ ë¶„ê¸°í•˜ê¸° ìœ„í•œ next_step ì„¤ì •.
        next_action = "generate"
    
    
    return {
        **state,
        "next_step": next_action
    }


def next_topic_question(state: InterviewState) -> InterviewState:
    """ìƒˆ ì£¼ì œì˜ ì²« ì§ˆë¬¸ì„ ì„ íƒí•˜ê³  current_questionì— ì„¤ì •"""
    
    question_queue = state.get("question_queue", [])
    remaining_topics = state.get("remaining_topics", [])
    strategy = state.get("question_strategy", {})

    # 1. í˜„ì¬ ì£¼ì œì˜ ë‚¨ì€ ì§ˆë¬¸ì´ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ë¨¼ì € ì†Œì§„ (Safety Check)
    if question_queue:
        new_question = question_queue.pop(0)
        current_strategy = state.get("current_strategy")

    # 2. ë‹¤ìŒ ì£¼ì œë¡œ ì „í™˜í•´ì•¼ í•  ê²½ìš°
    elif remaining_topics:
        current_topic_name = remaining_topics.pop(0) # ë‹¤ìŒ ì£¼ì œë¥¼ êº¼ëƒ„
        current_strategy = current_topic_name
        
        # ìƒˆ ì£¼ì œì˜ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ ì²« ì§ˆë¬¸ì„ ì„ íƒí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” íì— ë„£ìŒ
        questions = strategy.get("ë©´ì ‘ ì§ˆë¬¸ ì „ëµ", {}).get(current_topic_name, {}).get("ì˜ˆì‹œ ì§ˆë¬¸", [])
        if not questions:
            new_question = f"[{current_topic_name}] ì£¼ì œì— ëŒ€í•œ ì²« ì§ˆë¬¸ì…ë‹ˆë‹¤: í•´ë‹¹ ì£¼ì œ ê´€ë ¨ ê²½í—˜ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”."
        else:
            new_question = questions[0]
            question_queue = questions[1:]
    
    else: # ë‚¨ì€ ì£¼ì œê°€ ì—†ìŒ (route_nextì—ì„œ ì²˜ë¦¬ë˜ì–´ì•¼ í•˜ì§€ë§Œ ì•ˆì „ì¥ì¹˜)
        return {
            **state, 
            "next_step": "summarize",
            "current_question": "ë©´ì ‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤." # ì¢…ë£Œ ë©”ì‹œì§€
        }
    
    # 3. State ì—…ë°ì´íŠ¸
    return {
        **state,
        "current_question": new_question,
        "current_answer": "", # ë‹µë³€ì€ ì´ˆê¸°í™”
        "current_strategy": current_strategy,
        "question_queue": question_queue,
        "remaining_topics": remaining_topics,
        "generate_count": 0, # ì£¼ì œê°€ ë°”ë€Œë©´ ì‹¬í™” ì§ˆë¬¸ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
        "next_step": "evaluate" # ë‹¤ìŒ ì‹¤í–‰ì€ ë‹µë³€ì„ ê¸°ë‹¤ë¦° í›„ í‰ê°€ë¡œ ëŒì•„ê°€ì•¼ í•¨
    }


def generate_question(state: InterviewState) -> InterviewState:
    """ì‹¬í™” ì§ˆë¬¸ ìƒì„±: ì´ì „ í‰ê°€/ëŒ€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë”ìš± ì‹¬ë„ ìˆëŠ” ì§ˆë¬¸ì„ LLMì´ ìƒì„±"""
    
    resume_summary = state.get("resume_summary", "")
    resume_keywords = state.get("resume_keywords", [])
    question_strategy = state.get("question_strategy", {})
    conversation = state.get("conversation", [])
    evaluation = state.get("evaluation", [])
    generate_count = state.get("generate_count", 0) # ì‹¬í™” ì§ˆë¬¸ íšŸìˆ˜

    # 1. LLMì´ ì½ê¸° ì‰¬ìš´ ëŒ€í™” ê¸°ë¡ ë¬¸ìì—´ ìƒì„±
    history_str = ""
    for i, (conv, eval_item) in enumerate(zip(conversation, evaluation)):
        history_str += f"\n--- ì§ˆë¬¸ {i+1} ---\n"
        history_str += f"Q: {conv['question']}\n"
        history_str += f"A: {conv['answer']}\n"
        history_str += f"í‰ê°€: {json.dumps(eval_item['evaluation'], ensure_ascii=False)}\n"

    # 2. ì‹¬í™” ì§ˆë¬¸ì˜ ê¹Šì´ ì½”ë©˜íŠ¸ (generate_countë¥¼ í™œìš©í•œ ê³ ë„í™”)
    if generate_count <= 1:
        depth_comment = "ìµœê·¼ ë‹µë³€ì˜ ë¶€ì¡±í•œ ë¶€ë¶„ì„ ì±„ìš°ê±°ë‚˜, ê¸°ìˆ ì  ì´í•´ë„ë¥¼ ë” í™•ì¸í•  ìˆ˜ ìˆëŠ” ì‹¬í™” ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."
    elif generate_count == 2:
        depth_comment = "í˜„ì¬ ë‹µë³€ì˜ ë…¼ë¦¬ë‚˜ ê²½í—˜ì˜ êµ¬ì²´ì„±ì„ ê²€ì¦í•  ìˆ˜ ìˆëŠ” ë” ê¹Šì€ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”. (ì••ë°• ì§ˆë¬¸ í˜•íƒœë„ ê³ ë ¤)"
    else: # 3íšŒì°¨ ì´ìƒ
        depth_comment = "ì§€ê¸ˆê¹Œì§€ì˜ ë‹µë³€ì„ ì¢…í•©í•˜ì—¬, ì§€ì›ìì˜ ì‚¬ê³ ë ¥, ë¬¸ì œ í•´ê²°ë ¥, ê°€ì¹˜ê´€ì„ íƒêµ¬í•  ìˆ˜ ìˆëŠ” ê³ ë‚œë„ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."


    # 3. LLM í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_template(
        "\"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ AI ë©´ì ‘ê´€ì…ë‹ˆë‹¤.\n"
        "ì§€ì›ìì˜ ì—­ëŸ‰ì„ ì‹¬ì¸µì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•œ **ë‹¤ìŒ ì‹¬í™” ì§ˆë¬¸**ì„ í•˜ë‚˜ ìƒì„±í•´ ì£¼ì„¸ìš”.\n"
        "\n"
        "[ê·œì¹™]\n"
        "1. **ì ˆëŒ€ë¡œ** ì´ë ¥ì„œì˜ ì˜ˆì‹œ ì§ˆë¬¸ì´ë‚˜ ì´ì „ ë©´ì ‘ ê¸°ë¡ì— ë‚˜ì˜¨ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.\n"
        "2. {depth_comment} \n"
        "3. ì§ˆë¬¸ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.\n"
        "\n"
        "--- [ì§€ì›ì ì´ë ¥ì„œ ìš”ì•½] ---\n"
        "{summary}\n"
        "\n"
        "--- [ì§€ê¸ˆê¹Œì§€ì˜ ë©´ì ‘ ê¸°ë¡ (ì§ˆë¬¸, ë‹µë³€, í‰ê°€)] ---\n"
        "{history}\n"
        "\n"
        "--- [ë‹¤ìŒ ì‹¬í™” ì§ˆë¬¸ (ì˜¤ì§ ì§ˆë¬¸ í•˜ë‚˜ë§Œ ìƒì„±)]:\n"
        "\"\"\""
    )
    
    chain = prompt | llm
    
    # LLM í˜¸ì¶œí•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±
    response = chain.invoke({
        "summary": resume_summary,
        "history": history_str,
        "depth_comment": depth_comment
    })

    # 4. State ì—…ë°ì´íŠ¸
    return {
        **state,
        "current_question": response.content.strip(),
        "current_answer": "", # ë‹µë³€ì€ ì´ˆê¸°í™”
        "next_step": "evaluate" # ë‹¤ìŒ ì‹¤í–‰ì€ ë‹µë³€ì„ ê¸°ë‹¤ë¦° í›„ í‰ê°€ë¡œ ëŒì•„ê°€ì•¼ í•¨
    }


def preProcessing_Interview(file_path: str) -> InterviewState:
    """ë¯¸ì…˜ 1. ì‚¬ì „ ì¤€ë¹„ ì ˆì°¨ë¥¼ í•œ ë²ˆì— ì‹¤í–‰í•˜ê³  ì²« ì§ˆë¬¸ì„ ì„¤ì •í•©ë‹ˆë‹¤."""

    # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
    resume_text = extract_text_from_file(file_path)

    # 2. State ì´ˆê¸°í™”
    initial_state: InterviewState = {
        "resume_text": resume_text,
        "resume_summary": '',
        "resume_keywords": [],
        "question_strategy": {},
        "current_question": '',
        "current_answer": '',
        "current_strategy": '',
        "conversation": [],
        "evaluation": [],
        "next_step" : '',
        "question_queue": [],
        "remaining_topics": [],
        "generate_count": 0,
    }

    # 3. ì´ë ¥ì„œ ë¶„ì„
    state = analyze_resume(initial_state)

    # 4. ì§ˆë¬¸ ì „ëµ ìˆ˜ë¦½ ë° ì²« ì§ˆë¬¸ ì„¤ì • (generate_question_strategyì— í†µí•©)
    state = generate_question_strategy(state)
    
    # next_stepì€ ì´ˆê¸° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë°›ì•„ì•¼ í•˜ë¯€ë¡œ, ë‹¤ìŒ ë…¸ë“œëŠ” 'evaluate'ê°€ ë˜ì–´ì•¼ í•¨.
    state['next_step'] = 'evaluate' 

    return state

# --- LangGraph Graph Definition ---

def route_next(state: InterviewState) -> Literal["next_question", "generate", "next_topic", "summarize"]:
    """LLMì´ ê²°ì •í•œ next_stepì— ë”°ë¼ ë¶„ê¸°"""
    action = state["next_step"]
    
    if action == "next_question":
        return "next_question"
    elif action == "next_topic":
        return "next_topic"
    elif action == "generate":
        return "generate"
    else: # "summarize" or "end"
        return "summarize"

# ê·¸ë˜í”„ ì •ì˜ ì‹œì‘
workflow = StateGraph(InterviewState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("evaluate", evaluate_answer)
workflow.add_node("decide_next", decide_next_step)
workflow.add_node("next_question", next_topic_question) # ë‹¤ìŒ ì˜ˆì‹œ ì§ˆë¬¸ ë˜ëŠ” ì£¼ì œ ì „í™˜
workflow.add_node("next_topic", next_topic_question) # ì£¼ì œ ì „í™˜ (next_questionê³¼ ë™ì¼ í•¨ìˆ˜ ì‚¬ìš©)
workflow.add_node("generate", generate_question)
workflow.add_node("summarize", summarize_interview)

# ë…¸ë“œ ì—°ê²°
workflow.set_entry_point("evaluate") # ì²« ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ë¶€í„° ì‹œì‘

workflow.add_edge("evaluate", "decide_next")

workflow.add_conditional_edges(
    "decide_next",
    route_next,
    {
        "next_question": "next_question",
        "next_topic": "next_topic",
        "generate": "generate",
        "summarize": "summarize"
    }
)

workflow.add_edge("next_question", "evaluate")
workflow.add_edge("next_topic", "evaluate")
workflow.add_edge("generate", "evaluate")
workflow.add_edge("summarize", END)

graph = workflow.compile()
